"""
api_fastapi.py â€” Tupick RAG (FastAPI ë°±ì—”ë“œ)
===========================================
Streamlit ë‹¨ì¼ì•±(main.py)ì„ **FastAPI API ì„œë²„**ë¡œ ë³€í™˜í•œ ë²„ì „ì…ë‹ˆë‹¤.

ê¸°ëŠ¥ ìš”ì•½
- POST /ingest : URL ë‹¤ê±´ ìˆ˜ì§‘ â†’ ì²­í¬ â†’ ì„ë² ë”© â†’ FAISS ì¸ë±ìŠ¤(ì¦ë¶„) â†’ ì €ì¥
- POST /query  : ë²¡í„°ê²€ìƒ‰ + BM25 ì¬ìˆœìœ„ â†’ LLM ë¦¬í¬íŠ¸(A~H í…œí”Œë¦¿, ì„±í–¥ ë°˜ì˜)
- GET  /health : ìƒíƒœ/ë°±ì—”ë“œ í™•ì¸
- GET  /stats  : ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜
- (ì„ íƒ) GET /docs : ìë™ API ë¬¸ì„œ (uvicorn ì‹¤í–‰ ì‹œ)


ì‹¤í–‰
-----
    pip install fastapi uvicorn[standard] requests beautifulsoup4 sentence-transformers faiss-cpu rank-bm25 pydantic python-dotenv
    # (ì„ íƒ) OpenAI ì‚¬ìš© ì‹œ
    pip install openai

    # (ì„ íƒ) SPA(JS) ë Œë”ë§ì„ ì“°ë ¤ë©´
    pip install playwright && playwright install chromium

    uvicorn api_fastapi:app --reload
    # â†’ http://127.0.0.1:8000 /docs
    
    í„°ë¯¸ë„ì— ì…ë ¥í•˜ë©´ ì‹¤í–‰
    .\venv\Scripts\python.exe -m uvicorn api_fastapi:app --reload --host 127.0.0.1 --port 8000 

í™˜ê²½ ë³€ìˆ˜(.env)
---------------
- TUPICK_DATA_DIR (ê¸°ë³¸: Windowsë©´ C:\\tupick_data, ê·¸ ì™¸: ./tupick_data)
- EMB_MODEL_NAME (ê¸°ë³¸: BAAI/bge-m3)
- OPENAI_API_KEY / OPENAI_MODEL (ê¸°ë³¸: gpt-4o-mini)
- OLLAMA_HOST (ê¸°ë³¸: http://localhost:11434) / OLLAMA_MODEL (ê¸°ë³¸: llama3.1:8b-instruct)
- TOP_K (ê¸°ë³¸: 5)

ì£¼ì˜
----
- ê° ì‚¬ì´íŠ¸ì˜ ì•½ê´€/robots.txt/ì €ì‘ê¶Œì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
- ë³¸ ë°±ì—”ë“œëŠ” êµìœ¡/ì—°êµ¬ìš© ë°ëª¨ì´ë©°, íˆ¬ì ê¶Œìœ ê°€ ì•„ë‹™ë‹ˆë‹¤.
"""

from __future__ import annotations
import os
import re
import json
import uuid
import datetime
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import faiss
import requests
from bs4 import BeautifulSoup

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from fastapi.responses import HTMLResponse, Response

_profile_cache = {}

# === [ADD] User Profile ëª¨ë¸/ìŠ¤í† ë¦¬ì§€ ===
from pydantic import BaseModel
import json, os

class UserProfile(BaseModel):
    risk: str = "ì¤‘ê°„"
    budget: int = 1_000_000
    goal: str = "ë‹¨ê¸°í˜„ê¸ˆíë¦„"
    interest_field: str = "ë¶€ë™ì‚°, ì˜ˆìˆ , ìŒì•…"
    experience_level: str = "ì´ˆë³´ì"

DATA_DIR = os.path.join(os.getcwd(), "tupick_data")
PROFILE_PATH = os.path.join(DATA_DIR, "profile.json")

def load_profile():
    if os.path.exists(PROFILE_PATH):
        with open(PROFILE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return UserProfile().model_dump()

def save_profile(p):
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(PROFILE_PATH, "w", encoding="utf-8") as f:
        json.dump(p, f, ensure_ascii=False, indent=2)

profile_cache = load_profile()

# ----------------------------
# í™˜ê²½ì„¤ì • & ê²½ë¡œ
# ----------------------------
load_dotenv()

EMB_MODEL_NAME = os.getenv("EMB_MODEL_NAME", "BAAI/bge-m3")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TOP_K = int(os.getenv("TOP_K", 5))
CHUNK_SIZE = 550
CHUNK_OVERLAP = 60


def _default_data_dir() -> str:
    env = os.getenv("TUPICK_DATA_DIR")
    if env:
        return env
    if os.name == "nt":
        return r"C:\\tupick_data"  # Windows ASCII-safe ê²½ë¡œ (FAISS ìœ ë‹ˆì½”ë“œ ë²„ê·¸ íšŒí”¼)
    return os.path.join(os.getcwd(), "tupick_data")

DATA_DIR = _default_data_dir()
INDEX_DIR = os.path.join(DATA_DIR, "index")
DOCS_PATH = os.path.join(DATA_DIR, "docs.jsonl")

os.makedirs(INDEX_DIR, exist_ok=True)

# ----------------------------
# ë°ì´í„° ìŠ¤í‚¤ë§ˆ
# ----------------------------
class DocChunk(BaseModel):
    id: str
    source: str
    category: str
    title: str
    section: str
    url: str
    as_of_date: str
    text: str
    language: str = "ko-KR"


class IngestReq(BaseModel):
    urls: List[str]
    source: str = Field("generic")
    use_js: bool = Field(False, description="SPA/ì°¨ë‹¨ í˜ì´ì§€ JS ë Œë” ì‚¬ìš©")


class QueryReq(BaseModel):
    question: str
    risk: str = "ì¤‘ê°„"
    budget: int = 1_000_000
    goal: str = "ë‹¨ê¸°í˜„ê¸ˆíë¦„"
    k: int = 5
    use_profile: bool = True   


class QueryRes(BaseModel):
    answer: str
    backend: str
    k: int
    sources: List[Dict[str, Any]]

# ----------------------------
# ìœ í‹¸/ì „ì²˜ë¦¬
# ----------------------------

def now_date() -> str:
    return datetime.datetime.now().strftime("%Y-%m-%d")


def normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    toks = normalize_ws(text).split(" ")
    out, i = [], 0
    while i < len(toks):
        out.append(" ".join(toks[i:i + size]))
        i += max(1, size - overlap)
    return [c for c in out if len(c) > 20]


def guess_category_from_url(url: str) -> str:
    host = (requests.utils.urlparse(url).netloc or "").lower()
    if "tessa" in host:
        return "ë¯¸ìˆ í’ˆ/ëª…í’ˆ"
    if "music" in host:
        return "ì €ì‘ê¶Œ/ìŒì›"
    if "kasa" in host:
        return "ë¶€ë™ì‚°"
    return "ì¡°ê°íˆ¬ì/ì¼ë°˜"


def fetch_url(url: str, timeout: int = 20, use_js: bool = False) -> Tuple[str, str]:
    """1ì°¨: requests+BS4, ë³¸ë¬¸ì´ ì‘ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ(use_js=True) Playwright í´ë°±"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
        "Accept-Language": "ko,ko-KR;q=0.9,en-US;q=0.8",
        "Cache-Control": "no-cache",
    }
    try:
        r = requests.get(url, headers=headers, timeout=timeout)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        title = soup.title.text.strip() if soup.title else url
        for t in soup(["script", "style", "noscript"]):
            t.extract()
        body = normalize_ws(soup.get_text(" "))
        if use_js and len(body) < 800:
            raise RuntimeError("Body too small; try JS render")
        return title, body
    except Exception as e:
        if not use_js:
            raise
        # JS ë Œë” í´ë°±
        try:
            from playwright.sync_api import sync_playwright
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                ctx = browser.new_context(locale="ko-KR", user_agent=headers["User-Agent"])
                page = ctx.new_page()
                page.goto(url, wait_until="networkidle", timeout=timeout * 1000)
                title = page.title() or url
                body = page.locator("body").inner_text(timeout=timeout * 1000)
                browser.close()
                return title, normalize_ws(body)
        except Exception as e2:
            raise RuntimeError(f"fetch_url failed for {url}: {e2}")


def save_jsonl(items: List[Dict[str, Any]], path: str):
    with open(path, "a", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")


def load_jsonl(path: str) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

# ----------------------------
# ì„ë² ë”©/ì¸ë±ìŠ¤ (ê¸€ë¡œë²Œ in-memory + ë””ìŠ¤í¬ ì €ì¥)
# ----------------------------
_embedder: Optional[SentenceTransformer] = None
_index: Optional[faiss.IndexFlatIP] = None
_texts: List[str] = []
_metas: List[Dict[str, Any]] = []
_bm25: Optional[BM25Okapi] = None


def get_embedder() -> SentenceTransformer:
    global _embedder
    if _embedder is None:
        _embedder = SentenceTransformer(EMB_MODEL_NAME)
    return _embedder


def encode_texts(texts: List[str]) -> np.ndarray:
    emb = get_embedder()
    vecs = emb.encode(texts, normalize_embeddings=True)
    return np.array(vecs, dtype="float32")


def build_index(vectors: np.ndarray) -> faiss.IndexFlatIP:
    faiss.normalize_L2(vectors)
    index = faiss.IndexFlatIP(vectors.shape[1])
    index.add(vectors)
    return index


def index_paths() -> Tuple[str, str]:
    return os.path.join(INDEX_DIR, "faiss.index"), os.path.join(INDEX_DIR, "meta.json")


def save_index(index: faiss.IndexFlatIP):
    faiss_path, meta_path = index_paths()
    try:
        faiss.write_index(index, faiss_path)
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump({"count": len(_texts)}, f, ensure_ascii=False, indent=2)
    except Exception:
        # Windows í•œê¸€ ê²½ë¡œ ì´ìŠˆ í´ë°±
        fb_dir = os.path.join(r"C:\\tupick_data", "index") if os.name == "nt" else INDEX_DIR
        os.makedirs(fb_dir, exist_ok=True)
        faiss.write_index(index, os.path.join(fb_dir, "faiss.index"))
        with open(os.path.join(fb_dir, "meta.json"), "w", encoding="utf-8") as f:
            json.dump({"count": len(_texts)}, f, ensure_ascii=False, indent=2)


def load_index() -> Optional[faiss.IndexFlatIP]:
    candidates = []
    if os.name == "nt":
        candidates.append(os.path.join(r"C:\\tupick_data", "index", "faiss.index"))
    faiss_path, _ = index_paths()
    candidates.append(faiss_path)
    for p in candidates:
        if os.path.exists(p):
            try:
                return faiss.read_index(p)
            except Exception:
                continue
    return None


def ensure_retriever():
    global _index, _bm25
    if _bm25 is None and _texts:
        _bm25 = BM25Okapi([t.split() for t in _texts])
    if _index is None and _texts:
        vecs = encode_texts(_texts)
        _index = build_index(vecs)
        save_index(_index)


def vec_search(query: str, k: int) -> List[int]:
    if _index is None or not _texts:
        return []
    qv = encode_texts([query])  # (1, dim)
    faiss.normalize_L2(qv)
    scores, idxs = _index.search(qv, max(k, 8))
    return idxs[0].tolist()


def bm25_rerank(query: str, candidates: List[int], k: int) -> List[int]:
    if not candidates:
        return []
    if _bm25 is None:
        return candidates[:k]
    scores = _bm25.get_scores(query.split())
    pairs = [(i, float(scores[i])) for i in candidates]
    pairs.sort(key=lambda x: x[1], reverse=True)
    return [i for i, _ in pairs[:k]]

# ----------------------------
# LLM ë°±ì—”ë“œ
# ----------------------------
LLM_SYSTEM_PROMPT = (
    "ë„ˆëŠ” â€˜ì¡°ê°íˆ¬ì ë„ë©”ì¸ ì–´ì‹œìŠ¤í„´íŠ¸â€™ì•¼. ì œê³µëœ CONTEXTë§Œì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´. "
    "í•­ìƒ [ì‚¬ìš©ì ì„±í–¥]ì˜ ë¦¬ìŠ¤í¬, ì˜ˆì‚°, ëª©í‘œë¥¼ ë°˜ë“œì‹œ ë¦¬í¬íŠ¸ì— ë°˜ì˜í•´ì•¼ í•œë‹¤. "
    "ìˆ«ì/ê¸°ê°„/ìˆ˜ìµë¥  ë“±ì€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ â€˜ìë£Œ ë¶€ì¡±â€™ì´ë¼ê³  ë§í•´. "
    "íˆ¬ì ê¶Œìœ ì²˜ëŸ¼ ë‹¨ì •í•˜ì§€ ë§ê³  ì •ë³´ ì œê³µ ê´€ì ìœ¼ë¡œ ì‘ì„±í•´. "
    "ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ë¦¬í¬íŠ¸ í…œí”Œë¦¿ êµ¬ì¡°ë¡œ ì‘ì„±í•œë‹¤:\n"
    "A. í•µì‹¬ìš”ì•½ (3ì¤„ ì´ë‚´, ì‚¬ìš©ì ì„±í–¥ 1ì¤„ ë°˜ì˜)\n"
    "B. ìˆ˜ìµÂ·ë¹„ìš© êµ¬ì¡° ìš”ì•½ (ê·¼ê±° ë¬¸ì¥ ëì— [ë²ˆí˜¸])\n"
    "C. ì˜ˆì‚°ë³„ ì „ëµ (ì‚¬ìš©ì ì˜ˆì‚° ê¸°ì¤€ ë¶„í• /í‹°ì¼“/ìœ ë™ì„±)\n"
    "D. ë¦¬ìŠ¤í¬ í¬ì¸íŠ¸ (ì‚¬ìš©ì ë¦¬ìŠ¤í¬ ì„ í˜¸ ë°˜ì˜)\n"
    "E. ëª©í‘œ ì í•©ì„± í‰ê°€ (ì‚¬ìš©ì ëª©í‘œ ê¸°ì¤€, [ë²ˆí˜¸])\n"
    "F. ìœ ì‚¬ìƒí’ˆ ë¹„êµ (ìˆìœ¼ë©´, [ë²ˆí˜¸])\n"
    "G. ì¶œì²˜ (ë²ˆí˜¸, ì œëª©, URL)\n"
    "H. ì•ˆì „ ë¬¸êµ¬ (â€˜ë³¸ ì„œë¹„ìŠ¤ëŠ” ì •ë³´ ì œê³µìš©ì´ë©°, ìˆ˜ìµ ë³´ì¥ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€™)\n"
)


def build_user_prompt(question: str, passages: List[Dict[str, Any]], risk: str, budget: int, goal: str) -> str:
    ctx_lines = []
    for i, p in enumerate(passages, start=1):
        title = p.get("title", "Untitled"); url = p.get("url", ""); date = p.get("as_of_date", ""); text = p.get("text", "")
        ctx_lines.append(f"[{i}] {title} | {date} | {url}\n{text}")
    context = "\n\n".join(ctx_lines)
    return (
        f"[ì‚¬ìš©ì ì„±í–¥]\në¦¬ìŠ¤í¬={risk}, ì˜ˆì‚°={budget}ì›, ëª©í‘œ={goal}\n\n"
        f"[ì§ˆë¬¸]\n{question}\n\n"
        f"[CONTEXT]\n{context}\n"
    )


def call_ollama(prompt: str, system: str) -> str:
    url = f"{OLLAMA_HOST}/api/chat"
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        "stream": False,
        "options": {"temperature": 0.2},
    }
    r = requests.post(url, json=payload, timeout=120)
    r.raise_for_status()
    return r.json().get("message", {}).get("content", "")


def call_openai(prompt: str, system: str) -> str:
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.2,
    }
    r = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(payload), timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

# ----------------------------
# FastAPI ì•±
# ----------------------------
app = FastAPI(title="Tupick RAG API", version="0.2.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ë°°í¬ ì‹œ ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.on_event("startup")
def _startup():
    try:
        global _texts, _metas, _index, _bm25
        os.makedirs(DATA_DIR, exist_ok=True)
        os.makedirs(INDEX_DIR, exist_ok=True)
        
        # ê¸°ì¡´ ë¬¸ì„œ ë¡œë“œ (ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥ì„± ìˆëŠ” ë¶€ë¶„)
        rows = load_jsonl(DOCS_PATH)
        _texts = [r.get("text", "") for r in rows]
        _metas = [{k: r.get(k, "") for k in ["title","url","as_of_date","source","category","id","section"]} for r in rows]
        
        # ì¸ë±ìŠ¤/BM25 ì¤€ë¹„
        if _texts:
            _bm25 = BM25Okapi([t.split() for t in _texts])
            _index = load_index()
            if _index is None:
                vecs = encode_texts(_texts)
                _index = build_index(vecs)
                save_index(_index)
        
        print("âœ… ì„œë²„ ì‹œì‘ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âš ï¸ startup ì—ëŸ¬: {e}")
        # ì—ëŸ¬ê°€ ë‚˜ë„ ì„œë²„ëŠ” ì‹¤í–‰ë˜ë„ë¡
        _texts = []
        _metas = []
        _index = None
        _bm25 = None


@app.get("/health")
def health():
    backend = f"OpenAI Â· {OPENAI_MODEL}" if OPENAI_API_KEY else f"Ollama Â· {OLLAMA_MODEL}"
    return {"status": "ok", "docs": len(_texts), "index": _index is not None, "backend": backend}

# === [ADD] User Profile API ===
@app.get("/users/profile")
def get_profile():
    return profile_cache

@app.post("/users/profile")
def set_profile(p: UserProfile):
    global profile_cache
    profile_cache = p.model_dump()
    save_profile(profile_cache)
    return {"ok": True, "profile": profile_cache}

@app.get("/stats")
def stats():
    return {"docs": len(_texts)}


@app.post("/ingest")
def ingest(req: IngestReq):
    global _texts, _metas, _index, _bm25
    if not req.urls:
        raise HTTPException(status_code=400, detail="urls empty")
    items: List[Dict[str, Any]] = []
    added = 0
    for u in req.urls:
        try:
            title, body = fetch_url(u, use_js=req.use_js)
            cat = guess_category_from_url(u)
            for ch in chunk_text(body):
                did = f"{req.source}_{uuid.uuid4().hex[:12]}"
                items.append(DocChunk(
                    id=did, source=req.source, category=cat, title=title, section="ë³¸ë¬¸",
                    url=u, as_of_date=now_date(), text=ch
                ).model_dump())
        except Exception as e:
            # ê°œë³„ URL ì‹¤íŒ¨ëŠ” ê³„ì† ì§„í–‰
            continue
    if not items:
        raise HTTPException(status_code=400, detail="no items crawled")

    # ë””ìŠ¤í¬ì— append
    save_jsonl(items, DOCS_PATH)
    added = len(items)

    # ë©”ëª¨ë¦¬ ê°±ì‹ 
    new_texts = [it["text"] for it in items]
    new_metas = [{k: it.get(k, "") for k in ["title","url","as_of_date","source","category","id","section"]} for it in items]
    _texts.extend(new_texts)
    _metas.extend(new_metas)

    # BM25 ê°±ì‹  (ì „ì²´ ì¬ìƒì„±)
    _bm25 = BM25Okapi([t.split() for t in _texts])

    # FAISS ì¸ë±ìŠ¤ ê°±ì‹  (ì¦ë¶„: ì—†ìœ¼ë©´ ì‹ ì„¤, ìˆìœ¼ë©´ add)
    new_vecs = encode_texts(new_texts)
    if _index is None:
        _index = build_index(new_vecs)
    else:
        faiss.normalize_L2(new_vecs)
        _index.add(new_vecs)
    save_index(_index)

    return {"added": added, "total": len(_texts)}


@app.post("/query", response_model=QueryRes)
def query(req: QueryReq):
    ensure_retriever()
    if not _texts or _index is None:
        raise HTTPException(status_code=400, detail="index empty; call /ingest first")

    # 1) ê²€ìƒ‰ â†’ ì¬ìˆœìœ„
    cand = vec_search(req.question, k=req.k)
    topk = bm25_rerank(req.question, cand, k=req.k)

    # 2) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    passages = []
    for i in topk:
        if 0 <= i < len(_texts):
            meta = _metas[i]
            passages.append({**meta, "text": _texts[i]})

    # 3) í”„ë¡¬í”„íŠ¸ & ë°±ì—”ë“œ ì„ íƒ
    # í”„ë¡œí•„ ë³‘í•©
    prof = profile_cache
    risk = req.risk or prof.get("risk", "ì¤‘ê°„")
    budget = req.budget or prof.get("budget", 1_000_000)
    goal = req.goal or prof.get("goal", "ë‹¨ê¸°í˜„ê¸ˆíë¦„")
    
    if req.use_profile:
        prof = _profile_cache or load_profile()
        risk = req.risk or prof.get("risk", "ì¤‘ê°„")
        budget = req.budget or prof.get("budget", 1_000_000)
        goal = req.goal or prof.get("goal", "ë‹¨ê¸°í˜„ê¸ˆíë¦„")
    else:
        risk, budget, goal = req.risk, req.budget, req.goal


    prompt = build_user_prompt(req.question, passages, risk, budget, goal)
    print("ğŸ§­ Prompt preview:", prompt[:400])
    backend = "openai" if OPENAI_API_KEY else "ollama"

    # 4) LLM í˜¸ì¶œ
    try:
        if backend == "openai":
            answer = call_openai(prompt, LLM_SYSTEM_PROMPT)
        else:
            answer = call_ollama(prompt, LLM_SYSTEM_PROMPT)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM error: {e}")

    # 5) ì¶œì²˜ ëª©ë¡(ë²ˆí˜¸ ë§¤í•‘) ë°˜í™˜
    sources = []
    for j, p in enumerate(passages, start=1):
        sources.append({
            "n": j,
            "title": p.get("title", "Untitled"),
            "url": p.get("url", ""),
            "as_of_date": p.get("as_of_date", ""),
            "source": p.get("source", ""),
        })

    return QueryRes(
        answer=answer,
        backend=(f"OpenAI Â· {OPENAI_MODEL}" if OPENAI_API_KEY else f"Ollama Â· {OLLAMA_MODEL}"),
        k=len(passages),
        sources=sources,
    )
    
# === [ADD] Product Comparison API ===
class CompareReq(BaseModel):
    platforms: list[str] = ["ì¹´ì‚¬", "í…Œì‚¬", "ë®¤ì§ì¹´ìš°"]
    aspects: list[str] = ["ìˆ˜ìµë¥ ", "ë¶„ë°°ì£¼ê¸°", "ìµœì†Œíˆ¬ìê¸ˆ", "ë¦¬ìŠ¤í¬", "íŠ¹ì§•"]
    k: int = 5

@app.post("/compare", response_model=QueryRes)
def compare(req: CompareReq):
    ensure_retriever()
    if not _texts or _index is None:
        raise HTTPException(status_code=400, detail="index empty; call /ingest first")

    compare_q = f"{', '.join(req.platforms)}ì˜ {', '.join(req.aspects)}ë¥¼ í‘œë¡œ ë¹„êµí•´ì¤˜."
    cand = vec_search(compare_q, k=req.k)
    topk = bm25_rerank(compare_q, cand, k=req.k)

    passages = [{**_metas[i], "text": _texts[i]} for i in topk if 0 <= i < len(_texts)]

    prompt = (
        "[ë¹„êµ ìš”ì²­]\\n" + compare_q + "\\n\\n"
        "[CONTEXT]\\n" + "\\n\\n".join(
            f"[{i+1}] {p['title']} | {p['url']}\\n{p['text']}"
            for i, p in enumerate(passages)
        )
    )
    backend = "openai" if OPENAI_API_KEY else "ollama"
    answer = call_openai(prompt, LLM_SYSTEM_PROMPT) if backend == "openai" else call_ollama(prompt, LLM_SYSTEM_PROMPT)

    sources = [{"n": j+1, "title": p["title"], "url": p["url"]} for j, p in enumerate(passages)]
    return QueryRes(answer=answer, backend=backend, k=len(passages), sources=sources)


# ìƒë‹¨ importì— ì¶”ê°€
from fastapi.responses import HTMLResponse, Response, RedirectResponse
import os

from fastapi.staticfiles import StaticFiles
app.mount("/static", StaticFiles(directory="."), name="static")

@app.get("/", response_class=HTMLResponse)
def read_root():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return RedirectResponse(url="/docs")
    
# ì´ ë¼ìš°íŠ¸ ì¶”ê°€!
@app.get("/index.html", response_class=HTMLResponse)
def read_index_html():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return {"error": "index.html not found"}

@app.get("/app.html", response_class=HTMLResponse)
def read_app():
    try:
        with open("app.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return {"error": "app.html not found"}

@app.get("/about.html", response_class=HTMLResponse)
def read_about():
    try:
        with open("about.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return {"error": "about.html not found"}
    
@app.get("/features.html", response_class=HTMLResponse)
def read_features():
    try:
        with open("features.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return {"error": "features.html not found"}
    
@app.get("/how-it-works.html", response_class=HTMLResponse)
def read_how_it_works():
    try:
        with open("how-it-works.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return {"error": "how-it-works.html not found"}

@app.get("/favicon.ico")
def favicon():
    # íŒŒë¹„ì½˜ 404 ë¡œê·¸ ì—†ì• ê¸°ìš©
    return Response(status_code=204)



if __name__ == "__main__":
    import uvicorn
    uvicorn.run("api_fastapi:app", host="0.0.0.0", port=8000, reload=True)